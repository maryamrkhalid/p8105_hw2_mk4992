---
title: "P8105 HW2 – mk4992"
author: "Maryam Khalid"
date: "2025-10-01"
output:
  github_document:
    toc: true
    toc_depth: 2
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
set.seed(8105)

library(tidyverse)  
library(janitor)    
library(lubridate)  

## Problem 0: This homework follows the required reproducible workflow.
knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE,
  fig.path = "figs/"   # <— add this
)
```
## Problem 1: FiveThirtyEight US Politics Dataset

Step 1: clean the data in pols-month.csv
```{r pols-clean}
pols <- read_csv("data/pols-month.csv") |>
  clean_names() |> 
  separate(mon, into = c("year","month","day"), sep = "-", convert = TRUE) |> 
  mutate(
    month = month.name[month] |> tolower(),
    president = case_when(
      prez_gop == 1 ~ "gop",
      prez_dem == 1 ~ "dem",
      TRUE ~ NA_character_
    )
  ) |> 
  select(year, month, president, everything(), -prez_gop, -prez_dem, -day)

# View the cleaned dataset
pols |> slice_head(n = 10)
```

Step 2: clean the data in snp.csv
```{r snp-clean}
snp <- read_csv("data/snp.csv") |>
  clean_names() |> 
  separate(date, into = c("month","day","year"), sep = "/", convert = TRUE) |> 
  mutate(
    year = if_else(year <50, 2000 + year, 1900 + year), #modifies 2-digit years into 4-digits
    month = month.name[month] |> tolower(),
  ) |>
  arrange(year, match(month, tolower(month.name))) |>
  select(year, month, close)

# View the cleaned dataset
snp |> slice_head(n = 10)
```

Step 3: Clean the data in unemployment.csv

```{r unemp-clean}
unemp <- read_csv("data/unemployment.csv") |>
  clean_names() |>
  pivot_longer(
    cols = -year,
    names_to  = "month",
    values_to = "unemployment_rate"
  ) |>
  mutate(
    # map jan→january, feb→february, etc.
    month = case_match(
      month,
      "jan" ~ "january",   "feb" ~ "february", "mar" ~ "march",
      "apr" ~ "april",     "may" ~ "may",      "jun" ~ "june",
      "jul" ~ "july",      "aug" ~ "august",   "sep" ~ "september",
      "oct" ~ "october",   "nov" ~ "november", "dec" ~ "december"
    ),
    # ensure numeric (percent as a number, not character)
    unemployment_rate = as.numeric(unemployment_rate)
  ) |>
  arrange(year, match(month, tolower(month.name)))

# quick peek
unemp |> slice_head(n = 12)
```

Step 4: merge snp into pols, and then merge unemployment into the result

```{r joins}
# 1) merge S&P into pols
pols_snp <- left_join(pols, snp, by = c("year", "month"))

# 2) merge unemployment into the result
pols_snp_unemp <- left_join(pols_snp, unemp, by = c("year", "month"))

# quick sanity checks
dim(pols_snp_unemp)          # rows, cols
sum(is.na(pols_snp_unemp$close))            
sum(is.na(pols_snp_unemp$unemployment_rate))

pols_snp_unemp |> slice_head(n = 6)

```

```{r p1-describe}
# dimensions + year ranges
p1_dim_pols  <- dim(pols)
p1_dim_snp   <- dim(snp)
p1_dim_unemp <- dim(unemp)
p1_dim_final <- dim(pols_snp_unemp)

p1_yrs_pols  <- range(pols$year, na.rm = TRUE)
p1_yrs_snp   <- range(snp$year,  na.rm = TRUE)
p1_yrs_unemp <- range(unemp$year, na.rm = TRUE)
p1_yrs_final <- range(pols_snp_unemp$year, na.rm = TRUE)

# quick NA counts you can reference if you want
p1_na_close  <- sum(is.na(pols_snp_unemp$close))
p1_na_unemp  <- sum(is.na(pols_snp_unemp$unemployment_rate))
```

The `pols-month` dataset has `r p1_dim_pols[1]` rows and `r p1_dim_pols[2]` columns spanning `r p1_yrs_pols[1]`–`r p1_yrs_pols[2]`.  It gives of information about the # of federal politicians who are Democratic or Republican at any given time (# of governors, senators, house representatives of each party & whether the president at the time was GOP/Dem).
The `snp` dataset has `r p1_dim_snp[1]` rows and `r p1_dim_snp[2]` columns (`r p1_yrs_snp[1]`–`r p1_yrs_snp[2]`). It gives us observations related to the S&P stock market index, giving us the date of the observation & the closing value of the S&P stock index on that given date.
The `unemp` dataset has `r p1_dim_unemp[1]` rows and `r p1_dim_unemp[2]` columns (`r p1_yrs_unemp[1]`–`r p1_yrs_unemp[2]`).  The  first variable 'Year' contains the 4-digit year in its column, and the other variables are the months Jan through Dec. The values below the month columns indicate the percentage of unemployment in that column's month in that row's year.
After merging, the final dataset has `r p1_dim_final[1]` rows and `r p1_dim_final[2]` columns over `r p1_yrs_final[1]`–`r p1_yrs_final[2]`. Our final 'pols_snp_unemp' dataset allows us to track over time (by each month-year combination): the closing S&P index value, the unemployment rate, and the number of elected politicians of each party in office in various federal positions. Thus, we can take a birds-eye view of the economic and political state of the US at a given month-year in time. 

## Problem 2: Mr. Trash Wheel Dataset

```{r tw-sheets, message=FALSE}
library(readxl)
tw_path <- file.path("data", "202509 Trash Wheel Collection Data.xlsx")

```

Step 1: Clean the datasets

```{r tw-mr, message=FALSE}
library(dplyr)
library(janitor)
library(stringr)
library(readxl)

#Mr Trash Wheel
mr <- read_excel(tw_path, sheet = "Mr. Trash Wheel", skip = 1) |>
  clean_names() |>
  select(!contains("notes")) |>
  filter(!is.na(dumpster)) |>
  mutate(across(any_of("sports_balls"), ~ as.integer(round(.x)))) |>
  mutate(
    wheel = "Mr. Trash Wheel",
    year  = as.integer(year),
    month = str_to_title(as.character(month))
  )

mr |> slice_head(n = 5)

#Professor Trash Wheel
prof <- read_excel(tw_path, sheet = "Professor Trash Wheel", skip = 1) |>
  clean_names() |>
  select(!contains("notes")) |>
  filter(!is.na(dumpster)) |>
  mutate(
    across(any_of("sports_balls"), ~ as.integer(round(.x)))  # only if present
  ) |>
  mutate(
    wheel = "Professor Trash Wheel",
    year  = as.integer(year),
    month = str_to_title(as.character(month))
  )

#Gwynn Falls Trash Wheel
gwyn <- read_excel(tw_path, sheet = "Gwynns Falls Trash Wheel", skip = 1) |>
  clean_names() |>
  select(!contains("notes")) |>
  filter(!is.na(dumpster)) |>
  mutate(across(any_of("sports_balls"), ~ as.integer(round(.x)))) |>
  mutate(
    wheel = "Gwynnda",
    year  = as.integer(year),
    month = str_to_title(as.character(month))
  )

gwyn |> slice_head(n = 5)

```


Step 2: Combine the datasets altogether

```{r tw-combine}
trash <- bind_rows(mr, prof, gwyn) |>
  select(where(~ !all(is.na(.)))) |>
  select(
    wheel, dumpster, month, year, weight_tons, volume_cubic_yards,
    everything()
  ) |>
  arrange(wheel, year, match(month, month.name))

# sanity checks
trash |> count(wheel)
range(trash$year, na.rm = TRUE)
trash |> slice_head(n = 5)
```

```{r tw-summaries}

# (a) Total weight (tons) collected by Professor Trash Wheel
prof_weight_total_tons <- trash |>
  dplyr::filter(wheel == "Professor Trash Wheel") |>
  dplyr::summarise(total_tons = sum(weight_tons, na.rm = TRUE)) |>
  dplyr::pull(total_tons)

# (b) Total cigarette butts collected by Gwynnda in June 2022
gwynnda_cigs_june_2022 <- trash |>
  dplyr::filter(wheel == "Gwynnda", year == 2022, month == "June") |>
  dplyr::summarise(total_cigs = sum(cigarette_butts, na.rm = TRUE)) |>
  dplyr::pull(total_cigs)

```

```{r p2-describe}
p2_n            <- nrow(trash)
p2_n_wheels     <- dplyr::n_distinct(trash$wheel)
p2_year_range   <- range(trash$year, na.rm = TRUE)
p2_wheel_counts <- trash |> dplyr::count(wheel, name = "n_obs")
```

After cleaning and combining the three datasets of these different trash wheel devices, the dataset has `r nrow(trash)` observations (one per dumpster). Key variables include `month`, `year`, `weight_tons`, `volume_cubic_yards`, and item counts such as `plastic_bottles`, `polystyrene`, `cigarette_butts`, `glass_bottles`, `plastic_bags`, `wrappers`, and `sports_balls`.

The combined dataset has `r p2_n` observations across `r p2_n_wheels` wheels, spanning `r p2_year_range[1]`–`r p2_year_range[2]`. By wheel: `r paste(paste0(p2_wheel_counts$wheel, " = ", p2_wheel_counts$n_obs), collapse = "; ")`.  


Professor Trash Wheel collected a total of `r round(prof_weight_total_tons, 1)` tons of trash. Gwynnda collected `r format(gwynnda_cigs_june_2022, big.mark = ",")` cigarette butts in June 2022.

## Problem 3: Zillow Observed Rent Index (ZORI) in NYC (Jan 2015-Aug 2024) Dataset
```{r zipcodes, message=FALSE}
library(tidyverse)
library(janitor)
library(lubridate)
library(readr)
library(dplyr)
library(stringr)

# ZIP code → county / neighborhood mapping
zip_raw <- read_csv("data/Zip Codes.csv", show_col_types = FALSE) |>
  clean_names()

# map NYC counties to their respective boroughs
county_to_borough <- function(x) {
  x_low <- stringr::str_to_lower(x)
  x_low <- stringr::str_remove(x_low, "\\s*county$")
  dplyr::case_when(
    x_low == "new york" ~ "Manhattan",
    x_low == "kings"    ~ "Brooklyn",
    x_low == "queens"   ~ "Queens",
    x_low == "bronx"    ~ "Bronx",
    x_low == "richmond" ~ "Staten Island",
    TRUE ~ NA_character_
  )
}

zips <- zip_raw |>
  mutate(
    zip = stringr::str_pad(as.character(zip_code), width = 5, pad = "0"),
    borough = county_to_borough(county)
  ) |>
  select(zip, neighborhood, borough, county, county_fips, state_fips)

zip_xwalk <- zips |>
  arrange(zip, neighborhood) |>
  group_by(zip) |>
  summarise(
    borough      = dplyr::first(stats::na.omit(borough),      default = NA_character_),
    neighborhood = dplyr::first(stats::na.omit(neighborhood), default = NA_character_),
    county       = dplyr::first(stats::na.omit(county),       default = NA_character_),
    .groups = "drop"
  )

names(zip_raw)

```

```{r zori-long, message=FALSE}
# Read Zillow wide file (keep original date column names so regex matches)
zori_wide <- read_csv("data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv", show_col_types = FALSE)

# Keep ZIP-level rows, make a 5-digit ZIP
zori_zip <- zori_wide |>
  filter(StateName == "NY", str_detect(tolower(RegionType), "zip")) |>
  mutate(zip = str_pad(as.character(RegionName), 5, pad = "0"))

#identify metadata (non-date) columns, treat everything else as date
meta_cols <- c("RegionID","SizeRank","RegionName","RegionType",
               "StateName","State","City","Metro","CountyName")

date_cols <- names(zori_zip)[grepl(
  "^(\\d{1,2}/\\d{1,2}/\\d{2,4}|\\d{4}-\\d{2}-\\d{2})$",
  names(zori_zip)
)]

#Pivot all date columns
zori_long <- zori_zip |>
  tidyr::pivot_longer(
    cols = all_of(date_cols),
    names_to  = "date",
    values_to = "zori"
  ) |>
  mutate(
    date = coalesce(lubridate::mdy(date), lubridate::ymd(date))
  ) |>
  arrange(zip, date) |>
  select(
    zip, date, zori,
    region_id = RegionID, size_rank = SizeRank, region_type = RegionType,
    state_name = StateName, state = State, city = City, metro = Metro,
    county_name = CountyName
  )

# Quick peek
zori_long |> slice_head(n = 6)

# 1) Join first
tmp <- zori_long |>
  left_join(zip_xwalk, by = "zip")

# 2) Ensure a 'county' column exists (inject NA if it doesn't)
if (!"county" %in% names(tmp)) tmp$county <- NA_character_

# 3) Coalesce to prefer crosswalk county, fallback to Zillow's county_name
final <- tmp |>
  mutate(
    county = dplyr::coalesce(county, county_name),
    year   = lubridate::year(date),
    month  = lubridate::month(date, label = TRUE, abbr = FALSE)
  ) |>
  select(
    zip, borough, neighborhood, county,
    date, year, month, zori,
    region_id, size_rank, region_type, city, metro, state_name, state
  ) |>
  arrange(zip, date)

# counts
n_obs_final   <- nrow(final)
n_zip_final   <- dplyr::n_distinct(final$zip)
n_neigh_final <- dplyr::n_distinct(final$neighborhood)


# zips present in crosswalk but missing in Zillow
zips_in_zori          <- zori_long |> dplyr::distinct(zip)
zips_only_crosswalk   <- dplyr::anti_join(zip_xwalk, zips_in_zori, by = "zip")


# Jan 2020 vs Jan 2021 drop table
jan2020 <- final |> dplyr::filter(date == as.Date("2020-01-31")) |> dplyr::select(zip, zori_2020 = zori)
jan2021 <- final |> dplyr::filter(date == as.Date("2021-01-31")) |> dplyr::select(zip, zori_2021 = zori)

drop_tbl <- jan2021 |>
  dplyr::inner_join(jan2020, by = "zip") |>
  dplyr::mutate(change_2020_to_2021 = zori_2021 - zori_2020) |>
  dplyr::left_join(dplyr::select(zip_xwalk, zip, borough, neighborhood), by = "zip") |>
  dplyr::arrange(change_2020_to_2021) |>
  dplyr::slice_head(n = 10) |>
  dplyr::select(zip, borough, neighborhood, zori_2020, zori_2021, change_2020_to_2021)

drop_tbl
```

```{r covid-drop, results='asis'}
knitr::kable(
  drop_tbl,
  digits = 1,
  caption = "Largest declines in ZORI from Jan 2020 to Jan 2021 (NYC ZIPs)"
)

final_clean <- final |> filter(!is.na(zori))
```

The merged Zillow–ZIP dataset has `r n_obs_final` observations (one row per ZIP×month) from January 2015 all through August 2024. It includes `r n_zip_final` unique ZIP codes and `r n_neigh_final` unique neighborhoods. The key variables are: `zip`, `borough`, `neighborhood`, `date`/`year`/`month`, and `zori` (Zillow Observed Rent Index), plus extra data (`region_id`, `size_rank`, `city`, `metro`, `state`).

There are `r nrow(zips_only_crosswalk)` ZIP codes that appear in the city ZIP crosswalk but have no ZORI values. Examples are listed above. The likely reasons for this situation includes: PO-box-only ZIPs, non-residential/commercial ZIPs, zip-code boundary changes over time.

The table above lists the 10 ZIP codes with the largest ZORI declines from January 2020 to January 2021 (with borough and neighborhood). These areas tend to be pricier, denser neighborhoods that saw emigration and rental concessions/decreased costs early in the pandemic.


